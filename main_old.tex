\documentclass{article}[12pt]
\usepackage[a4paper, margin=1in]{geometry}

% packages
%   formatting
%\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue}
\usepackage{xcolor}
%\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage{float}
%   math
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bbm}
%\usepackage{booktabs}
%\usepackage{mathptmx}
%\usepackage{verbatim}
%\usepackage{bm}
%   figures, tables, ...
\usepackage{algpseudocode}
\usepackage{algorithm}
%\usepackage{graphicx}

% python
\usepackage{listings}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\lstdefinestyle{Python}{
    showstringspaces=false,
    language        = Python,
    basicstyle      = \small\ttfamily,
    morekeywords = {as},
    keywordstyle    = \color{blue},
    stringstyle     = \color{darkgreen},
    commentstyle    = \color{darkgreen}\ttfamily,
	breaklines = true,
	postbreak=\text{$\hookrightarrow$\space},
	% style >>> and ... 
	%   see: https://tex.stackexchange.com/questions/326655/make-a-keyword-in-listings-enviorment
	alsoletter = {>,.} ,
    morekeywords = [2]{>>>,...},
    keywordstyle = [2]\color{cyan}\bfseries}

% bibliograpy
\usepackage{biblatex}
\addbibresource{ags.bib}
\addbibresource{main.bib}

% macros
\input ags.tex
\newcommand{\bvarepsabs}{\boldsymbol{\varepsilon}_\text{abs}}
\newcommand{\bvarepsrel}{\boldsymbol{\varepsilon}_\text{rel}}
\newcommand{\varepsabs}{\varepsilon_\text{abs}}
\newcommand{\varepsrel}{\varepsilon_\text{rel}}

\newcommand{\AGSComment}[1]{{\color{cyan} Aleksei: #1}}
\newcommand{\JRComment}[1]{{\color{violet} Jag: #1}}
\newcommand{\FJH}[1]{{\color{purple}#1}}
\newcommand{\scnote}[1]{ {\textcolor{blue}  {\mbox{SC: } #1}}}

% metadata
\title{Robust Approximation of Sensitivity Indices in QMCPy}
\author{Aleksei Sorokin, Jagadeeswaran Rathinavel}
\date{\today}


\begin{document}

\maketitle

\AGSNote{

\section*{Notes}

TODO 
\begin{itemize}
    \item make \texttt{SensitivityIndices} class in QMCPy
    \item change everything to be 0 indexed like Python
    \item be consistent with either $f$ or $\bf$ throughout, not both
    \item early on talk about how sensitivity indices require a function of multiple integrals
    \item start error analysis section by with motivating examples and how we are trying to develop adaptive algorithms for error on functions of integrals
    \item change title to be more general function of integrals and talk about how sensitivity indices are an important example
\end{itemize}
MISC
\begin{itemize}
    \item Efficient estimation of the ANOVA mean dimension, with an application to neural net classification. Christopher R. Hoyt Stanford University. Art B. Owen Stanford University December 2020
\end{itemize}
}

\begin{abstract}
Sobol' indices quantify the importance of a function's inputs to explaining the output's variance.  Normalized Sobol' indices, or sensitivity indices, have been used in a variety of applications for global sensitivity analysis. Monte Carlo methods present an efficient approach for approximating these importance scores. This work describes our extension of such techniques to support advanced error estimation for multi-dimensional arrays. These methods are implemented and exemplified using QMCPy \cite{QMCPy}, an open source Quasi-Monte Carlo library in Python. %QMCPy utilizes adaptive algorithms to select an appropriate number of samples so the approximation is guaranteed to be within an desired tolerance of the true sensitivity indices. 
\end{abstract}

\section{Introduction}

\AGSNote{
\begin{itemize}
    \item introduce sobol/sensitivity indices
    \item discuss how (Quasi-)Monte Carlo has been used for robust approximations
    \item existing literature and software
    \item introduce notation
\end{itemize}}

Sensitivity analysis quantifies how uncertainty in a functions output may be attributed to subsets of function inputs. 

\section{Sensitivity Indices}

Functional ANOVA (analysis of variance) decomposes an objective function $f \in L^2((0,1)^d)$ into the sum of orthogonal functions $(f_u)_{u \subseteq 1:d}$. Here $1:d$ denotes the set of all dimensions $\{1,\dots,d\}$ and $f_u \in L^2((0,1)^{\lvert u \rvert})$ denotes a sub-function dependent only on inputs $\bx_u = (x_j)_{j \in u}$. By construction, these sub-functions sum to the objective function so that
\begin{equation*}
    f(\bx) = \sum_{u \subseteq 1:d} f_u(\bx_u)
\end{equation*}
\cite[Appendix A]{mcbook}. The orthogonality of sub-functions enables the variance of $f$ to be decomposed into the sum of variances of sub-functions. Specifically, denoting the variance of $f$ by $\calV(f)=\sigma^2$, we may write
\begin{equation*}
    \sigma^2 = \sum_{u \subseteq 1:d} \sigma^2_u
\end{equation*}
where $\sigma^2_u = \calV(f_u)$ is the variance of sub-function $f_u$. The sub-variance $\sigma_u$ directly quantifies the variance of $f$ attributable to inputs $u \subseteq 1:d$.  The \emph{closed and total Sobol' indices},
\begin{equation}
    \label{eq:sobol_indices}
    \underline{\tau}_u^2 = \sum_{v \subset u} \sigma^2_v \quad \text{and} \quad 
    \overline{\tau}_u^2 = \sum_{v \cap u \neq \emptyset} \sigma^2_v,
\end{equation}
quantify the variance attributable to subsets of $u$ and subsets containing $u$ respectively. The \emph{sensitivity indices},
\begin{equation}
    \label{eq:sensitivity_indices_og}
    \underline{s}_u = \underline{\tau}_u^2/\sigma^2 \quad \text{and} \quad 
    \overline{s}_u = \overline{\tau}_u^2/\sigma^2,
\end{equation}
normalize the Sobol' indices to quantify the proportion of variance explained by a given subset of inputs. 

\section{Monte Carlo Methods}

Monte Carlo methods are well-suited to approximate the expected value a random variable. Suppose we would like to approximating the vector mean $\bmu = \bbE[f(X)]$ where $X\sim\calU(0,1)^d$\footnote{While the standard uniform choice for $X$ may seem restrictive, a variety of distributions are compatible in this framework after an appropriate variable transformation, see \AGSNote{cite} for details.} and  $f: [0,1]^d \to \bbR^\rho$ is a vectorized objective function such that $f(\cdot)=(f_1(\cdot),\dots,f_\rho(\cdot))$ and  $(f_j)_{j=1}^\rho \subset L^2((0,1)^d)$.  Crude Monte Carlo computes the approximation 
\begin{equation}
    \label{eq:mcapprox}
    \hat{\bmu} = \frac{1}{n}\sum_{i=1}^n f(\bx_i) \approx \int_{[0,1]^d}f(\bx)\D\bx = \bmu
\end{equation}
using $\bx_1,\dots,\bx_n \simiid \calU(0,1)^d$. The error in approximating $\bmu$ using the sample average of IID points is  $\calO(n^{-1/2})$. 

Alternatively, one may compute a Quasi-Monte Carlo (QMC) estimate by replacing the IID points in \eqref{eq:mcapprox} with the first $n$ nodes in a carefully coordinated low discrepancy sequence. QMC methods converge to the true mean $\bmu$ like $\calO(n^{-1+\delta})$ where $0 < \delta \ll 1/2$, a rate that is significantly faster than that for crude Monte Carlo. While the remainder of this article focuses on extending the functionality of existing QMC methods, we note that adapting these improvements to crude Monte Carlo methods is relatively straightforward. For a more carefully treatment of crude Monte Carlo and Quasi-Monte Carlo see \AGSNote{cite}. 

\section{Existing QMC Methods}

Low discrepancy (LD) sequences are the cornerstone of Quasi-Monte Carlo methods. Digital sequences and integration lattices are among the more popular LD sequences used throughout the QMC community. Both are often implemented in base $2$ to enable fast point generation using bitwise operations. When using base $2$, the first $2^m$ points in the digital sequence or integration lattice will exhibit nice uniformity properties \AGSNote{cite}. This motivates QMC methods that iteratively double the number of samples and update error estimates until a desired error tolerance is satisfied. Therefore, one of the key challenges in QMC methods, and Monte Carlo methods more generally, is error estimation.

For crude Monte Carlo with IID points, the error is often estimated using the Central Limit Theorem \AGSNote{cite}. However, the derived bounds rely on an estimated variance and only apply as $n$ goes to $\infty$. \citeauthor{cubmcg} developed guaranteed error bounds with IID nodes based on the Berry-Esseen Inequality \cite{cubmcg}. 

For QMC, error estimates have often been based on $R$ repeated IID randomizations of an LD sequences and the corresponding $R$ estimates of $\bmu$ \AGSNote{cite}. However, such methods often require a prohibitively large number of function evaluations and often lack theoretically guaranteed error bounds. \citeauthor{cubqmclattice} overcome these challenges by developing algorithms that track the decay of Fourier coefficients based on a single randomized LD sequence \cite{adaptive_qmc}. These algorithms provide guaranteed error bounds on $\bmu$ for functions lying within an appropriately parameterized cone by tracking the decay of either the Walsh coefficients for digital sequences \cite{cubqmcsobol} or the complex exponential Fourier coefficients for integration lattices \cite{cubqmclattice}.  Another set of QMC algorithms take a Bayesian approach to error estimation. Rather than assume the function lies within a cone, these algorithms assume the integrand is an instance of a Gaussian process. Utilizing the properties of LD sequences allows fast approximation of Gaussian process hyperparameters and guarantees on error bounds. These Bayesian QMC algorithms are also available for both digital nets \AGSNote{cite} and integration lattices  \cite{cubqmcbayeslattice}. The above error estimation techniques are implemented, among others, into the open source QMC library QMCPy. Table \ref{table:qmcpy_sc} summarizes these implementations along with their compatibility and some features.

\begin{table}
\begin{tabular}{r c c c}
    QMCPy Class & Guaranteed Error Estimation & Compatible Points\\
    \hline
    \texttt{CubMCCLT} \AGSNote{cite} & & \texttt{IID} \\
    \texttt{CubMCG} \cite{cubmcg} & X & \texttt{IID} \\
    \texttt{CubQMCCLT} \AGSNote{cite} & & \texttt{LD} \\
    \texttt{CubQMCG} \cite{cubqmcsobol,cubqmclattice} & X & \texttt{DigitalNetB2}, \texttt{Lattice} \\
    \texttt{CubQMCBayesG} \AGSNote{cite} \cite{cubqmcbayeslattice} & X &  \texttt{DigitalNetB2}, \texttt{Lattice} \\
    \hline
\end{tabular}
\caption{A comparison of some of the Monte Carlo and Quasi-Monte Carlo stopping criterion algorithms available in QMCPy. \AGSNote{TODO: write wrapper \texttt{CubQMCG} and \texttt{CubQMCBayesG}. Write subclasses IID and LD}}
\label{table:qmcpy_sc}
\end{table}

\section{Monte Carlo Error Analysis}

Given an appropriate set of  samples and their corresponding function evaluations, the QMC stopping criterion discussed in this article can produce bounds on the individual means $\bmu \in \bbR^\rho$ such that $\bmu \in [\bp^-,\bp^+]$ either guaranteed or with high probability. We are interested in approximating a combined solution $s \in \bbR$ which is a function of individual means $\bmu$. To do so, we assume a user has defined $C^-,C^+,V: \bbR^\rho \times \bbR^\rho \to \bbR$ to be functions combining bounds on the individual integrand pieces $\bmu$ into a lower bound, upper bound, and violation flag on the combined solution $s \in \bbR$. Some examples are as follows: 
\begin{itemize}
    \item \emph{Ratio of Two Integrals:} Suppose one is interested in computing the combined solution $s = \mu_1/\mu_2$ from the individual solutions $\bmu = (\mu_1,\mu_2) \in \bbR^2$. Then appropriate lower and upper bounding functions on $s$ are 
    \begin{align*}
        C^-(\bp^-,\bp^+) &= \min_{\bmu \in [\bp^-,\bp^+]} \frac{\mu_1}{\mu_2} = \min\left(\frac{p_1^-}{p_2^-},\frac{p_1^+}{p_2^-},\frac{p_1^-}{p_2^+},\frac{p_1^+}{p_2^+}\right) \quad \text{and}\\
        C^+(\bp^-,\bp^+) &= \max_{\bmu \in [\bp^-,\bp^+]} \frac{\mu_1}{\mu_2} = \max\left(\frac{p_1^-}{p_2^-},\frac{p_1^+}{p_2^-},\frac{p_1^-}{p_2^+},\frac{p_1^+}{p_2^+}\right)
    \end{align*}
    given $p_2^-$ and $p_2^+$ have the same sign. To encode this requirement, we set the Boolean violation function to be 
    $$V(\bp^-,\bp^+) = \left(\text{sign}(p_2^-) \neq \text{sign}(p_2^+)\right).$$
    If the resulting flag is True, the QMC method is forced to perform another iteration with double the sample size. 
    \item \emph{Sensitivity Indices:} Suppose one is interested in computing the closed sensitivity index of $f$ at $u \in 1:d$. Then we may choose the individual solutions as $\bmu \in \bbR^3$ with
    \begin{align}
        \mu_1 &= \int_{[0,1]^d} f(\bx)\D\bx \label{eq:mu1}, \\
        \mu_2 &= \int_{[0,1]^d} \left(f(\bx)\right)^2\D\bx  \label{eq:mu2}, \quad \text{and} \\
        \mu_3 &= \underline{\tau}_u^2 = \int_{[0,1]^{2d}} f(\bx)[f(\bx_u,\bz_{-u})-f(\bz)]\D\bx\D\bz. \label{eq:tau_closed}
    \end{align}
    Moreover, an appropriate combined solution would be
    \begin{equation*}
        \label{eq:closed_si}
        \underline{s}_u = \frac{\underline{\tau}_u^2}{\sigma^2} = \frac{\mu_3}{\mu_2-\mu_1^2}
    \end{equation*}
    with reference to equations \eqref{eq:sobol_indices} and \eqref{eq:sensitivity_indices_og}. Above we have used the notation
    \begin{equation}
        (\bx_{u},\bz_{-u}) = \left(\begin{aligned}x_{j}, \quad & j \in u \\ z_{j}, \quad & j \notin u \end{aligned}\right)_{j=1}^d
    \end{equation}
    to denote a point with inputs $u$ from $\bx$ and inputs $-u=(1:d)\cap u^c$ from $\bz$. If we were instead interested in the total sensitivity index, we may set 
    \begin{equation}
        \mu_3 = \overline{\tau}_u^2 = \frac{1}{2}\int_{[0,1]^{d+u}} [f(\bz)-f(\bx_u,\bz_{-u})]^2\D\bz\D\bx_u
    \label{eq:tau_total}
    \end{equation}
    and use 
    \begin{equation*}
        \overline{s}_u = \frac{\overline{\tau}_u^2}{\sigma^2} = \frac{\mu_3}{\mu_2-\mu_1}.
    \end{equation*}
    Note that \eqref{eq:tau_closed} and \eqref{eq:tau_total} are not unique expressions for Sobol' indices. These formulations were chosen to enable fast computation with shared function evaluations. 
    
    Regardless, appropriate choices for the bounding and violation functions are 
    
    \JRComment{$p_1, p_2$ and $p_3$ may need some introduction here}
    \begin{align*}
        C^-(\bp^-,\bp^+) =& \max\left(0, \frac{p_3^-}{p_2^+ - (p_1^-)^2} \right), \\
        C^+(\bp^-,\bp^+) =& \min\left(1, \frac{p_3^+}{p_2^- - (p_1^+)^2}\right), \quad \text{and} \\
        V(\bp^-,\bp^+) =& \left(p_2^+-(p_1^-)^2 \leq 0\right) | \left(p_2^--(p_1^+)^2 \leq 0\right) | \left(p_3^- \leq 0\right)
    \end{align*}
    because we know $\bmu>=0$ element-wise. 
    
    \AGSNote{Can violation function can be more lenient? \\ TODO: Fix implementation to include max, min, and correct $V$ function.}
\end{itemize}
The following section generalizes solutions to multi-dimensional arrays with special attention to challenges in conservative computation. 

% (\JRComment{$\hat{s}$ not defined before}
We now wish to derive an optimal solution approximation $\hat{s}$ ) with respect to some error threshold $\varepsilon$, error metric $h(s,\varepsilon)$, and Boolean error criterion $\calC(s,\hat{s},\varepsilon)$ which we deem False if and only if $\lvert s - \hat{s} \rvert \leq h(s,\varepsilon)$. QMC stopping criterion will continue to double the sample size until $V(\bp^-,\bp^+)$ is False and $\calC(s,\hat{s},\varepsilon)$ is  $\False$. Error metric options include
\begin{subequations}
\begin{align}
    h(s,\varepsilon) & = \varepsilon \quad &&\text{absolute error satisfied}, \label{eq:h_abs}\\
    h(s,\varepsilon) &= \max\left(\varepsabs,\lvert s \rvert \varepsrel \right) \quad &&\text{absolute or relative error satisfied, and } \label{eq:h_abs_or_rel} \\
    h(s,\varepsilon) &= \min\left(\varepsabs,\lvert s \rvert \varepsrel \right) \quad &&\text{absolute and relative error satisfied.} \label{eq:h_abs_and_rel}
\end{align}
\end{subequations}
Note that $\hat{s}$ is not necessarily the midpoint of the combined solution bounds:
\begin{equation}
    \hat{s} \neq \frac{s^-+s^+}{2} \quad \text{in general.}
\end{equation}

Define $g(s,\hat{s},\varepsilon)=\lvert s - \hat{s} \rvert -h(s,\varepsilon)$ and note that the error criterion is met, $\calC(s,\hat{s},\varepsilon) = \False$, if and only if 
\begin{equation}
    \max_{s \in [s^-,s^+]} g(s,\hat{s},\varepsilon) \leq 0.
\end{equation}
We assume $h(\cdot,\varepsilon)$ satisfies 
\begin{equation}
    \lvert h(s,\varepsilon) - h(\tilde{s},\varepsilon) \rvert \leq \lvert s - \tilde{s} \rvert \quad \text{for any } s,\tilde{s} \in \bbR.
\end{equation}
So for all $s \in [s^-,s^+]$ we have either

\begin{align}
    g(s^-,\hat{s},\varepsilon)-g(s,\hat{s},\varepsilon) 
    &= \lvert s^- - \hat{s} \rvert -h(s^-,\varepsilon) - \lvert s - \hat{s} \rvert  + h(s,\varepsilon) \\
    &\geq s - s^- - \lvert h(s,\varepsilon)-h(s^-,\varepsilon) \rvert  %\JRComment{\text{is it supposed to be} \lvert s - s^- \rvert} % because $s >= s^-$
    \\
    &\geq 0 \qquad \text{if } s^- \leq s \leq \hat{s}, \text{ or} \\
    g(s^+,\hat{s},\varepsilon)-g(s,\hat{s},\varepsilon) 
    &= \lvert s^+ - \hat{s} \rvert -h(s^+,\varepsilon) - \lvert s - \hat{s} \rvert  + h(s,\varepsilon) \\
    &\geq s^+ - s - \lvert h(s,\varepsilon)-h(s^+,\varepsilon) \rvert \\
    &\geq 0 \qquad \text{if } \hat{s} \leq s \leq s^+.
\end{align}
This means that $g(\cdot,\hat{s},\varepsilon)$ attains its maximum at either $s^-$ or $s^+$ so that
\begin{equation}
    \max_{s \in [s^-,s^+]} g(s,\hat{s},\varepsilon) = \max g(s^\pm,\hat{s},\varepsilon) = \max\left(g(s^-,\hat{s},\varepsilon) \;,\; g(s^+,\hat{s},\varepsilon)\right).
\end{equation}

The function $g(s^-,\cdot,\varepsilon)$ is monotonically decreasing for $\hat{s} < s^-$ and monotonically increasing for $\hat{s} > s^-$. Similarly, $g(s^+,\cdot,\varepsilon)$ is monotonically decreasing for $\hat{s} < s^+$ and monotonically increasing for $\hat{s} > s^+$. This means that the optimal choice of $\hat{s}$ to minimize $\max g(s^\pm,\hat{s},\varepsilon)$ lies in $[s^-,s^+]$ and satisfies
\begin{align}
    g(s^-,\hat{s},\varepsilon) &= g(s^+,\hat{s},\varepsilon) \\
    \therefore \quad \hat{s} - s^- - h(s^-,\varepsilon) &= s^+ - \hat{s} - h(s^+,\varepsilon) \\ 
    \therefore \quad \hat{s} &= \frac{1}{2}\left[ s^- + s^+ +h(s^-,\varepsilon) - h(s^+,\varepsilon) \right]. \label{eq:shat_opt}
\end{align}
Under this optimal choice of $\hat{s}$, 
\begin{equation}
    \label{eq:g_under_shat_opt}
    2 \max g(s^\pm,\hat{s},\varepsilon) =  s^+  -  s^-  - h(s^-,\varepsilon) - h(s^+,\varepsilon).
\end{equation}

If $s^- < 0 < s^+$, then \eqref{eq:h_abs_and_rel} cannot hold since $h(s,\varepsilon) \le \lvert s \rvert \varepsrel$, and so
\begin{equation}
2 \max g(s^\pm,\hat{s},\varepsilon) \ge s^+ (1 - \varepsrel) - s^- (1  - \varepsrel) = (s^+ - s^-)(1 - \varepsrel) > 0.
\end{equation}
If $s^- < 0 < s^+$ and $\varepsabs = 0$ for \eqref{eq:h_abs_or_rel}, then $h(s,\varepsilon) = \lvert s \rvert \varepsrel$, and again \eqref{eq:h_abs_and_rel} cannot be satisfied.

\section{Multi-Dimensional Array Implementation} \label{sec: Vectorized Implementation}

In the previous section we assumed that individual solutions $\bmu \in \bbR^\rho$ were used to compute a single combined solution $s \in \bbR$. In this section we relax these assumptions to allow individual solutions $M \in \bbR^{\brho}$ and combined solutions $S \in \bbR^{\bseta}$ where $\brho$ and $\bseta$ are positive integer vectors of arbitrary length specifying the shape of the individual and combined solutions respectively. 

To be sample efficient we define the dependency function $D: \{\True,\False\}^{\bseta} \to \{\True,\False\}^{\brho}$ which maps sufficient estimation flags on the combined solutions back to estimation flags on individual solutions. For example, when $S = M$ (not combining solutions) we set the dependency function to the identity. Now suppose that after $2^m$ samples the QMC algorithm determines that, for some multi-indices $0 < \bk,\bs \leq \bseta$, $S_{\bk}$ has been sufficiently but $S_{\bl}$ has not. Then, still assuming $D$ is the identity map, when the QMC algorithm needs to query $f$ at the next $2^m$ points, it can tell $f$ to evaluate the ${\bl}$ output but to not bother evaluating the ${\bk}$ output.

Consider vectorized sensitivity indices as a more nuanced example. Suppose we would like to compute closed and total sensitivity indices at $(u_j)_{j=1}^k \subset 1:d$ for objective function $\tilde{f}: [0,1]^d \to \bbR$. Then we may set $\bseta = (2,k)$ where column $j \leq k$ of $S$ is $\left(\underline{s}_{u_j},\overline{s}_{u_j}\right)^T$ from \eqref{eq:sensitivity_indices_og}. Moreover, we set $\brho = (2,k+1)$ where column $j \leq k$ of $M$ is $\left(\underline{\tau}_{u_j}^2,\overline{\tau}_{u_j}^2\right)^T$ from \eqref{eq:sobol_indices} and column $k+1$ is $(\mu_1,\mu_2)^T$ from  \eqref{eq:mu1} and \eqref{eq:mu2}. Explicitly,
\begin{align*}
    S &= \begin{pmatrix} 
        \underline{s}_{u_1} & \underline{s}_{u_2} & \dots & \underline{s}_{u_k} \\ 
        \overline{s}_{u_1} & \overline{s}_{u_2} & \dots & \overline{s}_{u_k}
        \end{pmatrix} 
        \quad \text{and} \\ 
    M &=  \begin{pmatrix}
        \underline{\tau}_{u_1}^2 & \underline{\tau}_{u_2}^2 & \dots & \underline{\tau}_{u_k}^2 & \mu_1 \\ 
        \overline{\tau}_{u_1}^2 & \overline{\tau}_{u_2}^2 & \dots & \overline{\tau}_{u_k}^2 & \mu_2
        \end{pmatrix}.
\end{align*}
Notice that $\underline{s}_{u_j}$ only depends on $\underline{\tau}_{u_j}^2$, $\mu_1$, and $\mu_2$ for $j \in 1:k$. Similarly, $\overline{s}_{u_j}$ only depends on $\overline{\tau}_{u_j}^2$, $\mu_1$, and $\mu_2$ for $j \in 1:k$. Thus, the dependency function should set individual flags to be the combined flags with an additional column that is True when any of the combined flags are True and False otherwise. Explicitly, for combined flags $B \in \{\True,\False\}^{2 \times k}$, we set 
$$D\left(\begin{pmatrix}B_{11} & B_{12} & \dots & B_{1k} \\ B_{21} & B_{22} & \dots & B_{2k}\end{pmatrix}\right) = \begin{pmatrix}B_{11} & B_{12} & \dots & B_{1k} & \text{any}(B) \\ B_{21} & B_{22} & \dots & B_{2k} & \text{any}(B) \end{pmatrix}.$$
In our QMCPy implementation we further generalize to allow objective functions $\tilde{f}:[0,1]^d \to \bbR^{\tilde{\brho}}$ so that we prepend $(2,k)$ to the individual and combined shapes to get $\brho = (2,k+1,\tilde{\brho})$ and $\bseta = (2,k,\tilde{\brho})$.

\section{The Complete QMC Algorithm}

Algorithm \ref{algo:qmcsc} details the typical QMC stopping criterion included in QMCPy. The cost of this algorithm is concentrated in evaluating $f$ at $2^m$ points with a cost we assume to be $\$2^m$. In practice, this cost may be smaller if $f$ can take advantage of not having to produce all outputs for every iteration based on individual compute flags. Moreover, the cost may be further reduced through parallel evaluation. Although not explicitly stated in Algorithm \ref{algo:qmcsc}, we may easily process $f$ in parallel. For functions which are quick to evaluate, the overhead of multi-processing often out weights the benefits. However, for function that are slow to evaluate, multi-processing may greatly reduce the required compute time.

\begin{algorithm}
    \caption{\ct{QMC Stopping Criterion}: QMC Stopping Criterion}
    Operations are element-wise where $\tilde{B}=\True$
    \label{algo:qmcsc}
    \begin{algorithmic}
    \Require $\bf: [0,1]^d \times \{\True,\False\}^{\brho} \to \bbR^{\brho}$, an objective function that takes in LD samples at which to evaluate and compute flags of which outputs need to be computed
    \Require $m_0$, where $2^{m_0}$ is the initial number of samples
    \Require $\text{gen}: \bbN \times \bbN \to [0,1]^{n \times d}$, an appropriate generator of randomized LD samples taking in starting index $n_\text{start}$ and final index $n_\text{end}$ (not inclusive) and returns the $n = n_\text{end} - n_\text{start}$ samples in those positions of the sequence. 
    \Require $C^-,C^+,V: \bbR^{\brho} \times \bbR^{\brho} \to \bbR^{\bseta}$, functions combining bounds on individual solutions into lower bounds, upper bounds, and violation flags for the combined solution
    \Require $\bvarepsilon \in \bbR_+^{\bseta}$, a non-negative error tolerance 
    \Require $D: \{\True,\False\}^{\bseta} \to \{\True,\False\}^{\brho}$, a dependency function mapping sufficient estimation flags on the combined solutions to estimation flags on individual solutions. 
    \Require $h: \bbR^{\bseta} \times \bbR^{\bseta}_+ \to \bbR^{\bseta}$, an error metric function, see for example \eqref{eq:h_abs}, \eqref{eq:h_abs_or_rel}, or \eqref{eq:h_abs_and_rel}
    \Require $\text{update\_mu\_bounds}: \bbR^{\brho} \to \bbR^{\brho} \times \bbR^{\brho}$, a function which updates and returns individual error bounds based on the new input function evaluations
    
    \hrulefill
    
    \State $n_\text{start} \gets 0$
    \State $n_\text{end} \gets 2^{m_0}$
    \State $\tilde{B} \gets \True^{\bseta}$
    \State $B \gets \True^{\brho}$
    \While{$\text{any}(\tilde{B}) = \True$}
        \State $\bx \gets \text{gen}(n_\text{start},n_\text{end})$
        \State $\by \gets \bf(\bx,B)$
        \State $\bp^+,\bp^- \gets \text{update\_mu\_bounds}(\by)$
        \State $\bp^- = C^-(\bp^-,\bp^+)$
        \State $\bp^+ = C^+(\bp^-,\bp^+)$
        \State $\tilde{V} = V(\bp^-,\bp^+)$
        \State $\hat{\bs} \gets \left[\bs^-+\bs^++h(\bs^-,\bvarepsilon)-h(\bs^+,\bvarepsilon)\right]/2$
        \State $\bg_\text{max} \gets \left[\bs^+-\bs^--h(\bs^-,\bvarepsilon)-h(\bs^+,\bvarepsilon)\right]/2$
        \State $\tilde{B} \gets \left(\bg_\text{max}>0\right)$
        \State $B \gets D(\tilde{B} | \tilde{V})$
        \State $n_\text{start} \gets n_\text{end}$
        \State $n_\text{end} \gets 2n_\text{start}$
    \EndWhile
    \end{algorithmic}
\end{algorithm}

Sensitivity indices present an important special case for computational complexity. Say our QMC algorithm takes $2^m$ total samples to accurately approximate closed and total sensitivity indices for $(u_j)_{j=1}^k \subset 1:d$. Then the computational cost is $\$(2+k)2^m$ since every time our sensitivity index function is evaluated at $\tilde{\bx} = (\bx,\bz) \in [0,1]^{2d}$ we must evaluate the users objective function at $\bf(\bx)$, $f(\bz)$, and $f(\bx_{u_j},\bz_{-{u_j}})$ for $j=1,\dots,k$ to compute \eqref{eq:mu1}, \eqref{eq:mu2}, \eqref{eq:tau_closed}, and \eqref{eq:tau_total}.

If a user is only interested in approximating singleton sensitivity indices, $u_j = \{j\}$ for $j=1,\dots,d$, then it is possible to reduce the cost from $\$(2+d)2^m$ to $\$2^{m+1}$ using order $1$ replicated designs \cite{alex2008comparison,tissot2015randomized}. Such designs have been extended to  Sobol' sequences \AGSNote{(digital nets?)} in \cite{replicated_designs_sobol_seq} and utilized for sensitivity index approximation in \cite{reliable_sobol_indices_approx}. In the future, we plan to optimize QMCPy to utilize replicated designs when opportunities arise.

\section{Examples}

\subsection{Synthetic Function}

\AGSNote{Something for which sensitivity indies may be computed analytically, preferably with some use in literature.}

\subsection{Cantilever Beam Function}

\AGSNote{\url{https://www.sfu.ca/~ssurjano/canti.html}}

\subsection{Machine Learning}

\AGSNote{Maybe use the decision trees for iris dataset from \url{https://github.com/QMCSoftware/QMCSoftware/blob/iris/demos/iris.ipynb}. The results aren't that impressive since Iris is a pretty small dataset and sensitivity indices are somewhat automatic for decision trees. I'd like to do something like we have in this notebook though. This paper could include one or both of the analysis procedures. Hyperparameter importance is interesting, but currently doesn't have a lot of actionable results. Feature importance is more interesting / relevant, I'm leaning towards this. Art Owen and Chris Hoyt have done something similar for neural networks which is pretty interesting. Including both would be ideal (but time consuming).}

\section{Conclusions and Future Work}

\printbibliography

\end{document}
