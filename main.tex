\documentclass{article}

% packages
%   formatting
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{float}
%   math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{mathptmx}
\usepackage{verbatim}
%\usepackage{bm}
%   figures, tables, ...
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}

% python
\definecolor{darkgreen}{rgb}{0,0.6,0}
\lstdefinestyle{Python}{
    showstringspaces=false,
    language        = Python,
    basicstyle      = \small\ttfamily,
    morekeywords = {as},
    keywordstyle    = \color{blue},
    stringstyle     = \color{darkgreen},
    commentstyle    = \color{darkgreen}\ttfamily,
	breaklines = true,
	postbreak=\text{$\hookrightarrow$\space},
	% style >>> and ... 
	%   see: https://tex.stackexchange.com/questions/326655/make-a-keyword-in-listings-enviorment
	alsoletter = {>,.} ,
    morekeywords = [2]{>>>,...},
    keywordstyle = [2]\color{cyan}\bfseries}

% bibliograpy
\usepackage{biblatex}
\addbibresource{ags.bib}
\addbibresource{main.bib}

% macros
\input ags.tex
\newcommand{\FJH}[1]{{\color{purple}#1}}
\newcommand{\scnote}[1]{ {\textcolor{blue}  {\mbox{SC: } #1}}}
\newcommand{\bvarepsabs}{\boldsymbol{\varepsilon}_\text{abs}}
\newcommand{\bvarepsrel}{\boldsymbol{\varepsilon}_\text{rel}}
\newcommand{\varepsabs}{\varepsilon_\text{abs}}
\newcommand{\varepsrel}{\varepsilon_\text{rel}}

% metadata
\title{Robust Approximation of Sensitivity Indices in QMCPy}
\author{Aleksei Sorokin, Jagadeeswaran Rathinavel}
\date{\today}


\begin{document}

\maketitle

\section{Abstract}

Sobol' indices quantify the importance of a function's inputs to explaining the output's variance \cite[Appendix A]{mcbook}.  Normalized Sobol' indices, or sensitivity indices, have been used in a variety of applications for global sensitivity analysis. Monte Carlo methods present an efficient approach for approximating these importance scores. We will describe our implementation of such techniques into QMCPy \cite{QMCPy}, an open source Quasi-Monte Carlo library in Python. QMCPy utilizes algorithms from \cite{reliable_sobol_indices_approx} to adaptively select an appropriate number of samples so the approximation is guaranteed to be within an desired tolerance of the true sensitivity indices. 

\section{Background}

Functional ANOVA (analysis of variance) decomposes a function $f \in \calL^2(0,1)^d$ into the sum of orthogonal functions
\begin{equation}
    f(\bx) = \sum_{u \in 1:d} f_u(\bx_u)
\end{equation}
\cite[Appendix A]{mcbook} where $1:d = \{1,\dots,d\}$ and $f_u$ only depends on $\bx_u = (x_j)_{j \in u}$. Let us denote the variance of $f$ by $\sigma^2$ so that 
\begin{equation}
    \sigma^2 = \calV(f) = \mu_2 - \mu_1^2
\end{equation}
where
\begin{equation}
    \mu_1 = \int_{[0,1]^d}f(\bx)\D\bx \quad \text{and} \quad \mu_2 = \int_{[0,1]^d} f(\bx)^2 \D \bx.
\end{equation} 
The orthogonality of terms implies that the variance may be written in terms of sub-functions as
\begin{equation}
    \sigma^2 = \sum_{u \in 1:d} \sigma^2_u
\end{equation}
where $\sigma_u^2 = \calV(f_u)$. 

The closed and total \emph{Sobol' indices},
\begin{equation}
    \label{eq:sobol_indices}
    \underline{\tau}_u^2 = \sum_{v \subset u} \sigma^2_v \quad \text{and} \quad 
    \overline{\tau}_u^2 = \sum_{v \cap u \neq \emptyset} \sigma^2_v,
\end{equation}
quantify the variance attributable to subsets of $u$ and subsets containing $u$ respectively. The \emph{sensitivity indices},
\begin{equation}
    \label{eq:sensitivity_indices}
    \underline{s}_u = \underline{\tau}_u^2/\sigma^2 \quad \text{and} \quad 
    \overline{s}_u = \overline{\tau}_u^2/\sigma^2,
\end{equation}
normalize the Sobol' indices to quantify the proportion of variance explained by a given subset of inputs. These sensitivity indices may be written, in non-unique forms, as
\begin{align}
    \underline{\tau}_u^2 &= \int_{[0,1]^{2d}} f(\bx)[f(\bx_u,\bz_{-u})-f(\bz)]\D\bx\D\bz \qquad \text{and} \\
    \overline{\tau}_u^2 &= \frac{1}{2}\int_{[0,1]^{d+u}} [f(\bz)-f(\bx_u,\bz_{-u})]^2\D\bz\D\bx_u.
\end{align}

Given $u \in 1:d$, a simultaneous Monte Carlo approximation of both sensitivity indices may draw $n$ samples $(\bx_1,\bz_1), \dots, (\bx_n,\bz_n) \sim \calU(0,1)^{2d}$, subset into $v_1,\dots,v_n \sim U(0,1)^d$ where
\begin{equation}
    \bv_i = (\bx_{i,u},\bz_{i,-u}) = \left(\begin{aligned}x_{ij}, \quad & j \in u \\ z_{ij}, \quad & j \notin u \end{aligned}\right)_{j=1}^d,
\end{equation}
and compute approximations
\begin{subequations}
\label{eq:int_approxs}
\begin{align}
    \hat{\mu}_1 &= \frac{1}{n} \sum_{i=1}^n f(\bx_i) \\ % first moment
    \hat{\mu}_2 &= \frac{1}{n} \sum_{i=1}^n f(\bx_i)^2 \\ % second moment
    \hat{\underline{\tau}}_u^2 &= \frac{1}{n} \sum_{i=1}^n f(\bx_i)[f(\bv_i) - f(\bz_i)], \\ % closed Sobol' index
    \hat{\overline{\tau}}_u^2 &= \frac{1}{2n} \sum_{i=1}^n [f(\bz_i)-f(\bv_i)]^2. % total Sobol' index 
\end{align}
\end{subequations}
We may approximate the closed and total sensitivity via
\begin{equation}
    \hat{\underline{s}}_u = \frac{\hat{\underline{\tau}}_u^2}{\hat{\mu}_2-\hat{\mu}_1} \quad \text{and} \quad 
    \hat{\overline{s}}_u = \frac{\hat{\overline{\tau}}_u^2}{\hat{\mu}_2-\hat{\mu}_1}.
\end{equation}
Approximating $\hat{\underline{s}}_u$ and $\hat{\overline{s}}_u$ with $n$ shared samples requires $\$3n$ where evaluation of $f$ costs $\$1$. 
%Note that at this stage we have suppressed approximation's dependence on $n$.

\section{Vectorized Computation}

We now wish to approximate the closed and total sensitivities for a vector of index sets $\bu = \left(u_k\right)_{k=1}^{\lvert \bu \rvert}$ where $u_k \subset 1:d$. Let the true mean of our integrals be a vector $\bmu$ such that 
\begin{equation}
    \label{eq:bmu}
    \bmu = \left(\mu_1,\mu_2,\underline{\tau}_{u_1}^2,\dots,\underline{\tau}_{u_{\lvert \bu \rvert}}^2,\overline{\tau}_{u_1}^2,\dots,\overline{\tau}_{u_{\lvert \bu \rvert}}^2\right).
\end{equation}
Moreover, denote our sample average Monte Carlo approximation of $\bmu$ by
\begin{equation}
    \label{eq:bmu_hat}
    \hat{\bmu} = \left(\hat{\mu}_1,\hat{\mu}_2,\hat{\underline{\tau}}_{u_1}^2,\dots,\hat{\underline{\tau}}_{u_{\lvert \bu \rvert}}^2,\hat{\overline{\tau}}_{u_1}^2,\dots,\hat{\overline{\tau}}_{u_{\lvert \bu \rvert}}^2\right).
\end{equation}

The vector of integral approximations $\hat{\bmu}$ contains estimates of the first and second moments followed by $\lvert \bu \rvert$ closed then $\lvert \bu \rvert$ total Sobol' indices. 

We may then define the length $\lvert \bu \rvert$ vectors of sensitivity indices as 
\begin{equation}
    \label{eq:bs_split}
    \underline{\bs}_{\bu} = \left(\frac{\mu_{2+k}}{\mu_2-\mu_1}\right)_{k=1}^{\lvert u \rvert} \quad \text{and} \quad 
    \overline{\bs}_{\bu} = \left(\frac{\mu_{2+\lvert \bu \rvert + k}}{\mu_2-\mu_1}\right)_{k=1}^{\lvert u \rvert}.
\end{equation}
and their estimates as 
\begin{equation}
    \label{eq:bs_hat_split}
    \hat{\underline{\bs}}_{\bu} = \left(\frac{\hat{\mu}_{2+k}}{\hat{\mu}_2-\hat{\mu}_1}\right)_{k=1}^{\lvert u \rvert} \quad \text{and} \quad 
    \hat{\overline{\bs}}_{\bu} = \left(\frac{\hat{\mu}_{2+\lvert \bu \rvert + k}}{\hat{\mu}_2-\hat{\mu}_1}\right)_{k=1}^{\lvert u \rvert}.
\end{equation}
Again, we stack the closed and total indices and their estimates to get length $2\lvert \bu \rvert$ vectors 
\begin{equation}
    \label{eq:bs}
    \bs = (\underline{\bs}_{\bu},\overline{\bs}_{\bu}) \in \bbR^{}
\end{equation}
and 
\begin{equation}
    \label{eq:bs_hat}
    \hat{\bs} = (\underline{\hat{\bs}}_{\bu},\overline{\hat{\bs}}_{\bu}) \in \bbR^{2\lvert \bu \rvert}.
\end{equation}

We have designed QMCPy to estimate both $\hat{\underline{\bs}}_{\bu}$ and $\hat{\overline{\bs}}_{\bu}$ given arbitrary $\bu$. However, if a user is only interested in approximating $\hat{\underline{\bs}}_{\bu}$ for $\bu$ a set of singletons, $\lvert u_k \rvert = 1$ for $k=1, \dots, \lvert \bu \rvert$, it is possible to reduce the $n$ sample approximation cost from $\$(2+\lvert \bu \rvert)n$ to $\$2n$ using order $1$ replicated designs \cite{alex2008comparison,tissot2015randomized}. Such designs have been extended to  Sobol' sequences \AGSNote{(digital nets?)} in \cite{replicated_designs_sobol_seq} and utilized for sensitivity index approximation in \cite{reliable_sobol_indices_approx}. QMCPy includes the underlying adaptive, guaranteed cubature algorithm \cite{cubqmcsobol} and extends the error analysis in Section \ref{sec: Generalized Error Estimation}. In the future, we plan to optimize QMCPy to utilize replicated designs when opportunities arise.

\section{Adaptive Quasi-Monte Carlo Stopping Criterion}
\label{sec: Adaptive QMC SC}

The Quasi-Monte Carlo (QMC) \emph{stopping criterion} algorithms included in QMCPy adaptively select the number of \emph{low discrepancy} samples, $n$, so that a user specified error criterion is met. These methods improve upon crude Monte Carlo in two aspects. First, while standard Monte Carlo uses IID (independent identically distributed) sampling nodes, QMC utilizes carefully coordinated LD (low discrepancy) sequences to enable significantly faster convergence to the true mean. While the error $\varepsilon$ shrinks at a rate $\calO(n^{-1/2})$ for standard Monte Carlo, Quasi-Monte Carlo enables the significantly faster convergence rate $\calO(n^{-1+\delta})$ where $\delta >0$ is arbitrarily small. Second, QMCPy's stopping criterion continue to sample the integrand until the QMC approximation falls within a user-specified error tolerance of the true mean. Quantifying the approximation error in such a manner requires assumptions on the integrand. QMC algorithms that achieve this typically utilize base $2$ LD sequences such as integration lattices or digital nets. Because these LD sequences exhibit better uniformity properties for powers of $2$ \AGSNote{cite}, the algorithms will continue to double the number of samples and approximate the error until the user specified error tolerance is met. We now describe a various stopping criterion in QMCPy

A first idea for QMC cubature is based on repeated randomizations and the central limit theorem (CLT). To apply CLT to LD sequences, we take $R$ randomizations of an LD sequence and treat the $R$ repeated means as independent samples. Algorithm \ref{alg:CubQMCCLT} describes the \ct{CubQMCCLT} stopping criterion.
\begin{algorithm}
    \caption{\ct{CubQMCCLT} \AGSNote{TODO}}\label{alg:CubQMCCLT}
    \begin{algorithmic}
    \Require $n \geq 0$
%     \Ensure $y = x^n$
%     \State $y \gets 1$
%     \State $X \gets x$
%     \State $N \gets n$
%     \While{$N \neq 0$}
%     \If{$N$ is even}
%         \State $X \gets X \times X$
%         \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
%     \ElsIf{$N$ is odd}
%         \State $y \gets y \times X$
%         \State $N \gets N - 1$
%     \EndIf
%     \EndWhile
    \end{algorithmic}
\end{algorithm}
This stopping criterion has two main shortcomings. First, CLT only applies as $n$ goes to $\infty$, which is not justified for our $R$ independent samples. Second, our estimation of the variance is not justified. While we combat this with an inflation factor, there is no guarantee the true variance is bounded by our heuristic approximation. 

One way to avoid these issues is to utilize a functional analysis approach rather than a probabilistic one. Hickernell and Rugama have developed algorithms that approximate the error tolerance based on the decay of the integrands Fourier coefficients \cite{adaptive_qmc}. The \ct{CubQMCNetG} and \ct{CubQMCLatticeG} algorithms are available in QMCPy to accommodate both digital nets by way of Walsh coefficients \cite{cubqmcsobol} and integration lattices by way of complex exponential Fourier coefficients \cite{cubqmclattice}. These algorithms provide guaranteed error approximation for integrands lying within a cone of functions that can be parameterized to trade off function inclusion for algorithm speed. 

Another set of QMC algorithms take a Bayesian approach to error estimation. Rather than assume the function lies within a cone, these stopping criterion assume the integrand is an instance of a Gaussian process. Utilizing the properties of LD sequences allows fast approximation of Gaussian process hyperparameters and guarantees on error bounds. These algorithms are available in QMCPy as \ct{CubQMCBayesNetG} for digital nets \AGSNote{cite} and \ct{CubQMCBayesLatticeG} for LD integration lattices \cite{cubqmcbayeslattice}. 

\section{Generalized Error Estimation} \label{sec: Generalized Error Estimation}

\AGSNote{
    \begin{itemize}
        \item Based on the \emph{Error Criteria} Overleaf project by Fred.
        \item vectorization seems clunky. Perhaps formulate unvectorized and tack vectorization back on at the end. 
    \end{itemize}
}

Given a set of $2^m$ LD samples, QMC stopping criterion produce bound on the true mean $\bmu$ such that
\begin{equation}
    \bmu \in [\bp^-,\bp^+], \quad \text{ either guaranteed or with high probability.}
\end{equation}
Let the user define $C: \bbR^\rho \times \bbR^\rho \to \bbR^\eta \times \bbR^\eta$ to be the function combining bounds on the integrand pieces $\bmu$ into bounds on the solution $\bs$. For example:
\begin{itemize}
    \item Standard Monte Carlo wishing to approximate $\bs = \bmu = \int_{[0,1]^d} f(\bx)\D\bx$ where $\bmu$ is a length $\rho$ vector will set $\eta = \rho$ and define 
    \begin{align}
        C(\bp^+,\bp^-) = (\bp^+,\bp^-).
    \end{align}
    \item Sensitivity indices have $\rho=2+2\lvert \bu \rvert$ bounds on $\bmu$ defined in \eqref{eq:bmu}
    and $\eta=2\lvert \bu \rvert$ bounds on the sensitivity indies $\bs$ defined in \eqref{eq:bs}. Moreover, bounded integrand pieces are combined using
    \begin{subequations}
    \label{eq:C_senstivity}    
    \begin{align}
        C 
        &= (C^-,C^+) \qquad \text{where}  \\
        C^-(\bp^-,\bp^+)
        &=
        \max\left(\min_{\bmu \in [\bp^-,\bp^+]} \frac{\bmu_{3:}}{\mu_2-\mu_1^2} \;,\; 0\right) \\
        &= \max\left( \frac{\bp_{3:}^-}{p_2^+ - (p_1^-)^2} \;,\; 0\right) \quad \text{and}\\
        C^-(\bp^-,\bp^+) &=
        \min\left(\max_{\bmu \in [\bp^-,\bp^+]} \frac{\bmu_{3:}}{\mu_2-\mu_1^2} \;,\; 1\right) \\
        &= \min\left( \frac{\bp_{3:}^+}{p_2^- - (p_1^+)^2} \;,\; 1\right).\\
    \end{align}
    \end{subequations}

\end{itemize}

Setting $C(\bp^-,\bp^+) = (\bs^-,\bs^+)$ to bound the combined solutions, we now wish to derive an optimal solution approximation $\hat{\bs}$ with respect to some error metric $h(\bs,\bvarepsilon)$ and Boolean criterion $\calC(\bs,\hat{\bs},\bvarepsilon)$.  \AGSNote{OK to reuse $\hat{\bs},\bvarepsilon$ notation?}. The QMC stopping criterion in QMCPy continue to double the sample size until $\calC(\bs,\hat{\bs},\bvarepsilon)=\False$, which we determine to occur if and only if $\lvert \bs - \hat{\bs} \rvert \leq h(\bs,\bvarepsilon)$. 

For notational simplicity we assume only one combined solution $s$ with desired error bound $\varepsilon$. Vectorization is clarified in Section \ref{sec: Efficient Stopping Criterion Implementation}. Error metric options include
\begin{subequations}
\begin{align}
    h(s,\varepsilon) & = \varepsilon \quad &&\text{absolute error satisfied}, \\
    h(s,\varepsilon) &= \max\left(\varepsabs,\lvert s \rvert \varepsrel \right) \quad &&\text{absolute or relative error satisfied, and } \label{eq:h_abs_or_rel} \\
    h(s,\varepsilon) &= \min\left(\varepsabs,\lvert s \rvert \varepsrel \right) \quad &&\text{absolute and relative error satisfied.} \label{eq:h_abs_and_rel}
\end{align}
\end{subequations}
Note that $\hat{\bs}$ is not necessarily the midpoint of the combined solution bounds:
\begin{equation}
    \hat{s} \neq \frac{s^-+s^+}{2} \quad \text{in general.}
\end{equation}
Define $g(s,\hat{s},\varepsilon)=\lvert s - \hat{s} \rvert -h(s,\varepsilon)$ and note that $\calC(s,\hat{s},\varepsilon)=\False$ if and only if 
\begin{equation}
    \max_{s \in [s^-,s^+]} g(s,\hat{s},\varepsilon) \leq 0.
\end{equation}
We assume $h(\cdot,\varepsilon)$ satisfies 
\begin{equation}
    \lvert h(s,\varepsilon) - h(\tilde{s},\varepsilon) \rvert \leq \lvert s - \tilde{s} \rvert \quad \text{for any } s,\tilde{s} \in \bbR^\eta.
\end{equation}
So for all $s \in [s^-,s^+]$ we have either
\begin{align}
    g(s^-,\hat{s},\varepsilon)-g(s,\hat{s},\varepsilon) 
    &= \lvert s^- - \hat{s} \rvert -h(s^-,\varepsilon) - \lvert s - \hat{s} \rvert  + h(s,\varepsilon) \\
    &\geq s - s^- - \lvert h(s,\varepsilon)-h(s^-,\varepsilon) \rvert \\
    &\geq 0 \qquad \text{if } s^- \leq s \leq \hat{s}, \text{ or} \\
    g(s^+,\hat{s},\varepsilon)-g(s,\hat{s},\varepsilon) 
    &= \lvert s^+ - \hat{s} \rvert -h(s^+,\varepsilon) - \lvert s - \hat{s} \rvert  + h(s,\varepsilon) \\
    &\geq s^+ - s - \lvert h(s,\varepsilon)-h(s^+,\varepsilon) \rvert \\
    &\geq 0 \qquad \text{if } \hat{s} \leq s \leq s^+.
\end{align}
This means that $g(\cdot,\hat{s},\varepsilon)$ attains its maximum at either $s^-$ or $s^+$ so that
\begin{equation}
    \max_{s \in [s^-,s^+]} g(s,\hat{s},\varepsilon) = \max g(s^\pm,\hat{s},\varepsilon) = \max\left(g(s^-,\hat{s},\varepsilon) \;,\; g(s^+,\hat{s}\varepsilon)\right).
\end{equation}

The function $g(s^-,\cdot,\varepsilon)$ is monotonically decreasing for $\hat{s} < s^-$ and monotonically increasing for $\hat{s} > s^-$. Similarly, $g(s^+,\cdot,\varepsilon)$ is monotonically decreasing for $\hat{s} < s^+$ and monotonically increasing for $\hat{s} > s^+$. This means that the optimal choice of $\hat{s}$ to minimize $\max g(s^\pm,\hat{s},\varepsilon)$ lies in $[s^-,s^+]$ and satisfies \AGSNote{Why?}
\begin{align}
    g(s^-,\hat{s},\varepsilon) &= g(s^+,\hat{s},\varepsilon) \\
    \therefore \quad \hat{s} - s^- - h(s^-,\varepsilon) &= s^+ - \hat{s} - h(s^+,\varepsilon) \\ 
    \therefore \quad \hat{s} &= \frac{1}{2}\left[ s^- + s^+ +h(s^-,\varepsilon) - h(s^+,\varepsilon) \right]. \label{eq:shat_opt}
\end{align}
Under this optimal choice of $\hat{s}$, 
\begin{equation}
    \label{eq:g_under_shat_opt}
    2 \max g(s^\pm,\hat{s},\varepsilon) =  s^+  -  s^-  - h(s^-,\varepsilon) - h(s^+,\varepsilon).
\end{equation}

If $s^- < 0 < s^+$, then \eqref{eq:h_abs_and_rel} cannot hold since $h(s,\varepsilon) \le \lvert s \rvert \varepsrel$, and so
\begin{equation}
2 \max g(s^\pm,\hat{s},\varepsilon) \ge s^+ (1 - \varepsrel) - s^- (1  - \varepsrel) = (s^+ - s^-)(1 - \varepsrel) > 0.
\end{equation}
If $s^- < 0 < s^+$ and $\varepsabs = 0$ for \eqref{eq:h_abs_or_rel}, then $h(s,\varepsilon) = \lvert s \rvert \varepsrel$, and again \eqref{eq:h_abs_and_rel} cannot be satisfied.

\section{Efficient Stopping Criterion Implementation}
\label{sec: Efficient Stopping Criterion Implementation}

QMCPy's stopping criteria support multiple layers of vectorization. First, the input function may be a vectorized function $\mathbf{f} = (f_1,\dots,f_{\dtilde} )$ where $f_{\tilde{j}} \in \calL(0,1)^d$ for $\tilde{j}=1,\dots,\dtilde$. For sensitivity indices, we may simply redefine $\bmu$ in \eqref{eq:bmu} to be an element of $\bbR^{\dtilde \times (2+2\lvert \bu \rvert)}$, or equivalently a vector in $\bbR^{2\dtilde+2\dtilde\lvert \bu \rvert}$ for QMCPy compatibility. Second, the final solution, perhaps combining multiple integrand approximations, may itself be a vector such as the one defined in  \eqref{eq:bmu_hat}.


QMCPy enables efficient computation via a vector of flags for the combined solutions that are appropriately propagated back to the underlying integrands. The following example illustrates this concept for sensitivity indices. Algorithm \ref{alg:QMCSL} describes an abstract computational procedure for finite dimensional QMC stopping criterion in QMCPy. 

\subsection{Example: Compute Flags for Sensitivity Indices} 

\AGSNote{
    Better to describe this in terms of vectors for clarity on QMCPy implementation or with matrices for intuitive indexing? 
}

Suppose $\mathbf{f}:\bbR^d \to \bbR^{\dtilde}$ and we want to approximate singleton sensitivity indices, $\bu = \{ \{1\}, \dots, \{d\} \}$, for each of the $\dtilde$ integrands. An initial approximation draws $n=2^m$ samples, evaluates $\mathbf{f}$ at the $2^m(2+d)$ points required for \eqref{eq:int_approxs}, and computes error bounds $\bp^-,\bp^+ \in \bbR^{\dtilde \times (2+2d)}$ on $\bmu$ using a method from  Section \ref{sec: Adaptive QMC SC}. Note that $\bmu \in \bbR^{\dtilde \times (2+2d)}$ is defined analogous to \eqref{eq:bmu} where $\bmu_{:,1}$ are the first moments, $\bmu_{:,2}$ are second moments, $\bmu_{:,3:(2+d}$ are total Sobol' indices and $\bmu_{:,(3+d):(2+2d)}$ are closed Sobol' indices. If indexing in \eqref{eq:C_senstivity} specified columns, then we could compute sensitivity index error bounds $\bs^-,\bs^+ \in \bbR^{\dtilde \times d}$ and optimal solution approximation $\hat{\bs}$ using \eqref{eq:shat_opt} applied element wise.  We can then apply \eqref{eq:g_under_shat_opt} element wise to construct Boolean matrix $B = \calC(\bs,\hat{\bs},\bvarepsilon) \in \{\True,\False\}^{\dtilde \times d}$ indicating which sensitivity index approximations fail to satisfy the desired error tolerance. 

Let $\calB \in \{\True,\False\}^{\dtilde \times (2+d)}$ be a Boolean matrix indicating whether approximations of $\bmu$ fail to satisfy the desired tolerance. Noting that $B_{\jtilde,k}$  depends on $\mu_{\jtilde,k+2}$, $\mu_{\jtilde,1}$, and $\mu_{\jtilde,2}$ by \eqref{eq:C_senstivity}, we may set $\calB_{:,3:}=B$ and then set $\calB_{\jtilde,1:2} = \text{any}(\calB_{\jtilde,3:(2+d)})$ for $\jtilde=1,\dots,\dtilde$. In the above example we have used matrix notation to clarify the dependency structure. In QMCPy, sensitivity index approximation replaces 
$\mathbf{f} : [0,1]^d \to \bbR^{\dtilde}$ with
$\mathbf{f}_{\text{new}}: [0,1]^d \to \bbR^{(2+\lvert \bu \rvert)\dtilde}$
and reshapes where necessary.

\begin{algorithm}
    \caption{\ct{QMCSL}: QMC Single Level Stopping Criterion \AGSNote{TODO}}\label{alg:QMCSL}
    \begin{algorithmic}
     \Require $h$
%     \Ensure $y = x^n$
%     \State $y \gets 1$
%     \State $X \gets x$
%     \State $N \gets n$
%     \While{$N \neq 0$}
%     \If{$N$ is even}
%         \State $X \gets X \times X$
%         \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
%     \ElsIf{$N$ is odd}
%         \State $y \gets y \times X$
%         \State $N \gets N - 1$
%     \EndIf
%     \EndWhile
    \end{algorithmic}
\end{algorithm}

\section{Additional QMCPy Features}

\AGSNote{
\begin{itemize}
    \item Multiple stopping criteria, both MC and QMC
    \item Variation reduction techniques such as
    \begin{itemize}
        \item importance sampling
        \item control variates
    \end{itemize}
    \item vectorized integrands $\boldsymbol{f}$
\end{itemize}
}

\section{Metalearning Studies}

\subsection{Gradient Boosted Trees}

\subsection{Neural Networks}


\section{Conclusions and Future Work}

\AGSNote{
\begin{itemize}
    \item Implement Tony's replicated designs when $U$ is compatible
\end{itemize}
}

\printbibliography

\end{document}
