\documentclass{article}
\usepackage[utf8]{inputenc}

% packages
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{booktabs}
%\usepackage{bm}
\usepackage{mathptmx}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}

% hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,}

% python
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\lstdefinestyle{Python}{
    showstringspaces=false,
    language        = Python,
    basicstyle      = \small\ttfamily,
    morekeywords = {as},
    keywordstyle    = \color{blue},
    stringstyle     = \color{darkgreen},
    commentstyle    = \color{darkgreen}\ttfamily,
	breaklines = true,
	postbreak=\text{$\hookrightarrow$\space},
	% style >>> and ... 
	%   see: https://tex.stackexchange.com/questions/326655/make-a-keyword-in-listings-enviorment
	alsoletter = {>,.} ,
    morekeywords = [2]{>>>,...},
    keywordstyle = [2]\color{cyan}\bfseries}

% bibliograpy
\usepackage{biblatex}
\addbibresource{ags.bib}
\addbibresource{main.bib}

% macros
\input ags.tex
\newcommand{\FJH}[1]{{\color{purple}#1}}
\newcommand{\scnote}[1]{ {\textcolor{blue}  {\mbox{SC: } #1}}}
\newcommand{\bepsabs}{\boldsymbol{\epsilon}_\text{abs}}
\newcommand{\bepsrel}{\boldsymbol{\epsilon}_\text{rel}}
\newcommand{\bepsabs}{\epsilon_\text{abs}}
\newcommand{\bepsabs}{\epsilon_\text{rel}}

% metadata
\title{Robust Approximation of Sensitivity Indices in QMCPy}
\author{Aleksei Sorokin, Jagadeeswaran Rathinavel}
\date{\today}


\begin{document}

\maketitle

\section{Abstract}

Sobol' indices quantify the importance of a function's inputs to explaining the output's variance \cite[Appendix A]{mcbook}.  Normalized Sobol' indices, or sensitivity indices, have been used in a variety of applications for global sensitivity analysis. Monte Carlo methods present an efficient approach for approximating these importance scores. We will describe our implementation of such techniques into QMCPy \cite{QMCPy}, an open source Quasi-Monte Carlo library in Python. QMCPy utilizes algorithms from \cite{reliable_sobol_indices_approx} to adaptively select an appropriate number of samples so the approximation is guaranteed to be within an desired tolerance of the true sensitivity indices. 

\section{Background}

Functional ANOVA (analysis of variance) decomposes a function $f \in \mathcal{L}^2(0,1)^d$ into the sum of orthogonal functions
\begin{equation}
    f(\bx) = \sum_{u \in 1:d} f_u(\bx_u)
\end{equation}
\cite[Appendix A]{mcbook} where $1:d = \{1,\dots,d\}$ and $f_u$ only depends on $\bx_u = (x_j)_{j \in u}$. This formulation allows the variance of $f$,
\begin{equation}
    \sigma^2 = \calV(f) =
    \int_{[0,1]^d} f(\bx)^2 \D \bx - \left(\int_{[0,1]^d}f(\bx)\D\bx\right)^2,
\end{equation} 
to be decomposed into the sum of variances of sub-functions
\begin{equation}
    \sigma^2 = \sum_{u \in 1:d} \sigma^2_u
\end{equation}
where $\sigma_u^2 = \calV(f_u)$. 

The closed and total \emph{Sobol' indices},
\begin{equation}
    \underline{\tau}_u^2 = \sum_{v \subset u} \sigma^2_v \quad \text{and} \quad 
    \overline{\tau}_u^2 = \sum_{v \cap u \neq \emptyset} \sigma^2_v,
\end{equation}
quantify the variance attributable to subsets of $u$ and subsets containing $u$ respectively. The \emph{sensitivity indices},
\begin{equation}
    \underline{s}_u = \underline{\tau}_u^2/\sigma^2 \quad \text{and} \quad 
    \overline{s}_u = \overline{\tau}_u^2/\sigma^2,
\end{equation}
normalize the Sobol' indices to quantify the proportion of variance explained by a given subset of inputs. These sensitivity indices may be written, in non-unique forms, as
\begin{align}
    \underline{\tau}_u^2 &= \int_{[0,1]^{2d}} f(\bx)[f(\bx_u,\bz_{-u})-f(\bz)]\D\bx\D\bz \qquad \text{and} \\
    \overline{\tau}_u^2 &= \frac{1}{2}\int_{[0,1]^{d+u}} [f(\bz)-f(\bx_u,\bz_{-u})]^2\D\bz\D\bx_u.
\end{align}

Given $u \in 1:d$, a simultaneous Monte Carlo approximation of both sensitivity indices may draw $n$ samples $(\bx_1,\bz_1), \dots, (\bx_n,\bz_n) \sim \calU(0,1)^{2d}$, subset into $v_1,\dots,v_n \sim U(0,1)^d$ where
\begin{equation}
    \bv_i = (\bx_{i,u},\bz_{i,-u}) = \left(\begin{aligned}x_{ij}, \quad & j \in u \\ z_{ij}, \quad & j \notin u \end{aligned}\right)_{j=1}^d,
\end{equation}
and compute approximations
\begin{align}
    \hat{\mu}_1 &= \frac{1}{n} \sum_{i=1}^n f(\bx_i) \\ % first moment
    \hat{\mu}_2 &= \frac{1}{n} \sum_{i=1}^n f(\bx_i)^2 \\ % second moment
    \hat{\underline{\tau}}_u^2 &= \frac{1}{n} \sum_{i=1}^n f(\bx_i)[f(\bv_i) - f(\bz_i)], \\ % closed Sobol' index
    \hat{\overline{\tau}}_u^2 &= \frac{1}{2n} \sum_{i=1}^n [f(\bz_i)-f(\bv_i)]^2. % total Sobol' index 
\end{align}
We may approximate the closed and total sensitivity via
\begin{equation}
    \hat{\underline{s}}_u = \frac{\hat{\underline{\tau}}_u^2}{\hat{\mu}_2-\hat{\mu}_1} \quad \text{and} \quad 
    \hat{\overline{s}}_u = \frac{\hat{\overline{\tau}}_u^2}{\hat{\mu}_2-\hat{\mu}_1}.
\end{equation}
Approximating $\hat{\underline{s}}_u$ and $\hat{\overline{s}}_u$ with $n$ shared samples requires $\$3n$ where evaluation of $f$ costs $\$1$. 
%Note that at this stage we have suppressed approximation's dependence on $n$.

\section{Vectorized Computation}

We now wish to approximate the closed and total sensitivities for a vector of index sets $\bu = \left(u_k\right)_{k=1}^{\lvert \bu \rvert}$ where $u_k \subset 1:d$. If $A: \bbR^{n \times 2d} \to \bbR^{2+2\lvert \bu \rvert}$ is to handle the approximation computation given a set of $n$ samples, then we set
\begin{equation}
    \label{eq:bmuhat_approx}
    A\left(\{\bx_i,\bz_i\}_{i=1}^n\right) = \hat{\bmu} = \left(\hat{\mu}_1,\hat{\mu}_2,\hat{\underline{\tau}}_{u_1}^2,\dots,\hat{\underline{\tau}}_{u_{\lvert \bu \rvert}}^2,\hat{\overline{\tau}}_{u_1}^2,\dots,\hat{\overline{\tau}}_{u_{\lvert \bu \rvert}}^2\right).
\end{equation}
The vector of integral approximations $\hat{\bmu}$ contains estimates of the first and second moments followed by $\lvert \bu \rvert$ closed then $\lvert \bu \rvert$ total sensitivity indices Sobol' indices. Now, the  estimates for closed and total indices be define as the length $\lvert \bu \rvert$ vectors
\begin{equation}
    \label{eq:sensitivity_indices_vec}
    \hat{\underline{\bs}}_{\bu} = \left(\frac{\hat{\mu}_{2+k}}{\hat{\mu}_2-\hat{\mu}_1}\right)_{k=1}^{\lvert u \rvert} \quad \text{and} \quad 
    \hat{\overline{\bs}}_{\bu} = \left(\frac{\hat{\mu}_{2+\lvert \bu \rvert + k}}{\hat{\mu}_2-\hat{\mu}_1}\right)_{k=1}^{\lvert u \rvert}.
\end{equation}

We have designed QMCPy to estimate both $\hat{\underline{\bs}}_{\bu}$ and $\hat{\overline{\bs}}_{\bu}$ given arbitrary $\bu$. However, if a user is only interested in approximating $\hat{\underline{\bs}}_{\bu}$ for $\bu$ a set of singletons, $\lvert u_k \rvert = 1$ for $k=1, \dots, \lvert \bu \rvert$, it would be possible to reduce the $n$ sample approximation cost from $\$(2+\lvert \bu \rvert)n$ to $\$2n$ using order $1$ replicated designs \cite{alex2008comparison,tissot2015randomized}. Order $1$ replicated designs are extended to Sobol' sequences \AGSNote{(digital nets?)} in \cite{replicated_designs_sobol_seq}. This extension was developed to facilitate the adaptive digital net cubature for sensitivity indices in \cite{reliable_sobol_indices_approx} which builds upon the algorithm in \cite{cubqmcsobol}. We have included the adaptive, guaranteed cubature algorithm of \cite{cubqmcsobol} in QMCPy and extend error analysis from \cite{reliable_sobol_indices_approx} in Section \ref{sec: Generalized Error Estimation}. In the future, we hope to optimize QMCPy to utilize replicated designs when opportunities arise.

\section{Adaptive Quasi-Monte Carlo Stopping Criterion}

The Quasi-Monte Carlo (QMC) \emph{stopping criterion} algorithms included in QMCPy adaptively select the number of \emph{low discrepancy} samples, $n$, so that a user specified error criterion is met. These methods improve upon crude Monte Carlo in two aspects. First, while standard Monte Carlo uses IID (independent identically distributed) sampling nodes, QMC utilizes carefully coordinated LD (low discrepancy) sequences to enable significantly faster convergence to the true mean. While the error $\epsilon$ shrinks at a rate $\calO(n^{-1/2})$ for standard Monte Carlo, Quasi-Monte Carlo enables the significantly faster convergence rate $\calO(n^{-1+\delta})$ where $\delta >0$ is arbitrarily small. Second, QMCPy's stopping criterion continue to sample the integrand until the QMC approximation falls within a user-specified error tolerance. Quantifying the approximation error in such a manner requires assumptions on the integrand. QMC algorithms that achieve this typically utilize base $2$ LD sequences such as integration lattices or digital nets. Because these LD sequences exhibit better uniformity properties for powers of $2$ \AGSNote{cite}, the algorithms will continue to double the number of samples and approximate the error until the user specified error tolerance is met. We now describe a various stopping criterion in QMCPy

A first idea for QMC cubature is based on repeated randomizations and the central limit theorem (CLT). To apply CLT to LD sequences, we take $R$ repeated randomizations of a LD sequence and apply CLT to the resulting $R$ mean estimates. Algorithm \ref{alg:CubQMCCLT} describes the \ct{CubQMCCLT} stopping criterion.
% \begin{algorithm}
%     \caption{\ct{CubQMCCLT} \AGSNote{TODO}}\label{alg:CubQMCCLT}
%     \begin{algorithmic}
%     \Require $n \geq 0$
%     \Ensure $y = x^n$
%     \State $y \gets 1$
%     \State $X \gets x$
%     \State $N \gets n$
%     \While{$N \neq 0$}
%     \If{$N$ is even}
%         \State $X \gets X \times X$
%         \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
%     \ElsIf{$N$ is odd}
%         \State $y \gets y \times X$
%         \State $N \gets N - 1$
%     \EndIf
%     \EndWhile
%     \end{algorithmic}
% \end{algorithm}
This stopping criterion has two main shortcomings. First, CLT only applies as $n$ goes to $\infty$, which is not justified for our discrete approximations. Second, our estimation of the variance is not justified. While we combat this with an inflation factor, there is no guarantee the true variance is bounded by our heuristic approximation. 

One way to avoid these issues is to utilize a functional analysis approach rather than a probabilistic one. Hickernell and Rugama have developed algorithms that approximate the error tolerance based on the decay of the integrands Fourier coefficients \cite{adaptive_qmc}. The \ct{CubQMCNetG} and \ct{CubQMCLatticeG} algorithms are available in QMCPy to accommodate both digital nets by way of Walsh coefficients \cite{cubqmcsobol} and integration lattices by way of complex exponential Fourier coefficients \cite{cubqmclattice}. These algorithms provide guaranteed error approximation for integrands lying within a cone of functions that can be parameterized to trade off function inclusion for algorithm speed. 

Another set of QMC algorithms take a Bayesian approach to error estimation. Rather than assume the function lies within a cone, these stopping criterion assume the integrand is an instance of a Gaussian process. Utilizing the properties of LD sequences allows fast approximation of Gaussian process hyperparameters and guarantees on error bounds. These algorithms are available in QMCPy as \ct{CubQMCBayesNetG} for digital nets \AGSNote{cite} and \ct{CubQMCBayesLatticeG} for LD integration lattices \cite{cubqmcbayeslattice}. 

\section{Generalized Error Estimation} \label{sec: Generalized Error Estimation}

\AGSNote{
    \begin{itemize}
        \item Based on the \emph{Error Criteria} Overleaf project by Fred.
        \item vectorization seems clunky. Perhaps formulate unvectorized and tack vectorization back on at the end. 
    \end{itemize}
}

Given a set of $2^m$ LD samples, QMC stopping criterion produce lower and upper bounds on the true means of integrands, $\bmu$. Let us call these bounds $\bp^-$ and $\bp^+$. Then 
\begin{equation}
    \bmu \in [\bp^-,\bp^+], \quad \text{ either guaranteed or with high probability.}
\end{equation}
Let the user define $C: \bbR^\rho \times \bbR^\rho \to \bbR^\eta \times \bbR^\eta$ to be the function combining bounds on the integrand pieces into bounds on the solution. For example:
\begin{itemize}
    \item Standard Monte Carlo wishing to approximate $\bmu = \int_{[0,1]^d} f(\bx)\D\bx$ where $\bmu$ is a length $\rho$ vector will set $\eta = \rho$ and define 
    \begin{align}
        C(\bp^+,\bp^-) = (\bp^+,\bp^-).
    \end{align}
    \item Sensitivity indices have $\rho=2+2\lvert \bu \rvert$ bounds corresponding to the integrands approximated in \eqref{eq:bmuhat_approx} and $\eta=2\lvert \bu \rvert$ bounds on the sensitivity indies approximated in \eqref{eq:sensitivity_indices_vec}. Moreover, bounded integrand pieces are combined using $C(\bp^-,\bp^+) = \left(C_j^-(\bp^-,\bp^+), C_j^+(\bp^-,\bp^+\right)_{j=1}^{2\lvert \bu \rvert}$ and 
    \begin{align}
        C_j^-(\bp^-,\bp^+) 
        &= \max\left(\min_{\bmu \in [\bp^-,\bp^+]} \frac{\mu_{2+j}}{\mu_2-\mu_1^2}, 0 \right) \\
        &= \max\left(\frac{p_{2+j}^-}{p_2^+-(p_1^-)^2}, 0\right), \\
        C_j^+(\bp^-,\bp^+) 
        &= \min\left(\max_{\bmu \in [\bp^-,\bp^+]} \frac{\mu_{2+j}}{\mu_2-\mu_1^2}, 1\right) \\
        & = \min\left(\frac{p_{2+j}^+}{p_2^- - (p_1^+)^2}, 1\right).
    \end{align}
\end{itemize}

Setting $C(\bp^-,\bp^+) = (\bs^-,\bs^+)$ to bound the combined solutions, we now wish to derive an optimal solution approximation $\hat{\bs}$ with respect to some error metric $h(\bs,\bepsilon)$ and Boolean criterion $\calC(\bs,\hat{\bs},\bepsilon)$.  \AGSNote{OK to reuse $\hat{\bs},\bepsilon$ notation?}. The QMC stopping criterion in QMCPy continue to double the sample size until $\calC(\bs,\hat{\bs})=\True$, which we determine to occur if and only if $\lvert \bs - \hat{\bs} \rvert \leq h(\bs,\bepsilon)$ where $\lvert \cdot \rvert$ is performed element wise. Error metric options include
\begin{align}
    h(\bs,\bepsilon) & = \bepsilon \quad &&\text{absolute error satisfied}, \\
    h(\bs,\bepsilon) &= \max\left(\bepsabs,\lvert \bs \rvert \bepsrel \right) \quad &&\text{absolute or relative error satisfied, and } \label{eq:h_abs_or_rel} \\
    h(\bs,\bepsilon) &= \min\left(\bepsabs,\lvert \bs \rvert \bepsrel \right) \quad &&\text{absolute and relative error satisfied.} \label{eq:h_abs_and_rel}
\end{align}
Note that $\hat{\bs}$ is not necessarily the midpoint of the combined solution bounds:
\begin{equation}
    \hat{\bs} \neq \frac{\bs^-+\bs^+}{2} \quad \text{in general.}
\end{equation}
Define $g(\bs,\hat{\bs},\bepsilon)=\lvert \bs - \hat{\bs}\rvert -h(\bs,\bepsilon)$ and note that $\calC(\bs,\hat{\bs})=\True$ if and only if 
\begin{equation}
    \max_{\bs \in [\bs^-,\bs^+]} g(\bs,\hat{\bs},\bepsilon) \leq 0.
\end{equation}
We assume $h(\cdot,\bepsilon)$ satisfies 
\begin{equation}
    \lvert h(\bs_2,\bepsilon) - h(\bs_1,\bepsilon) \rvert \leq \lvert \bs_2 - \bs_1 \rvert \quad \text{element wise for any } \bs_1,\bs_2 \in \bbR^\eta.
\end{equation}
So for all $\bs \in [\bs^-,\bs^+]$ and $j=1,\dots,\eta$ we have either
\begin{align}
    g(s_j^-,\hat{s}_j,\epsilon_j)-g(s_j,\hat{s}_j,\epsilon_j) 
    &= \lvert s_j^- - \hat{s}_j \rvert -h(s_j^-,\epsilon_j) - \lvert s_j - \hat{s}_j \rvert  + h(s_j,\epsilon_j) \\
    &\geq s_j - s_j^- - \lvert h(s_j,\epsilon_j)-h(s_j^-,\epsilon_j) \rvert \\
    &\geq 0 \quad \text{if } s_j^- \leq s_j \leq \hat{s}_j, \text{ or} \\
    g(s_j^+,\hat{s}_j,\epsilon_j)-g(s_j,\hat{s}_j,\epsilon_j) 
    &= \lvert s_j^+ - \hat{s}_j \rvert -h(s_j^+,\epsilon_j) - \lvert s_j - \hat{s}_j \rvert  + h(s_j,\epsilon_j) \\
    &\geq s_j^+ - s_j - \lvert h(s_j,\epsilon_j)-h(s_j^+,\epsilon_j) \rvert \\
    &\geq 0 \quad \text{if } \hat{s}_j \leq s_j \leq s_j^+.
\end{align}
This means that $g(\cdot,\hat{\bs},\bepsilon)$ attains its maximum on the boundary of $[\bs^-,\bs^+]$:
\begin{equation}
    \max_{\bs \in [\bs^-,\bs^+]} g(\bs,\hat{\bs},\bepsilon) = \max g(\bs^\pm,\hat{\bs},\bepsilon)
\end{equation}
where
\begin{equation}
    \max g(\bs^\pm,\hat{\bs},\bepsilon) = \left(\max\left(g(s_j^+,\hat{s}_j,\epsilon_j),g(s_j^-,\hat{s}_j,\epsilon_j)\right)\right)_{j=1}^\eta
\end{equation}
is element wise. 

The function $g(\bs^\pm,\cdot,\bepsilon)$ is monotonically decreasing for $\hat{\bs} < \bs^\pm$ and monotonically increasing for $\hat{\bs} > \bs^\pm$. This means that the optimal choice of $\hat{\bs}$ to minimize $\max g(\bs^\pm,\hat{\bs},\bepsilon)$ lies in $[\bs^-,\bs^+]$ and satisfies
\begin{align}
    g(\bs^-,\hat{\bs},\bepsilon) &= g(\bs^+,\hat{\bs},\bepsilon) \\
    \therefore \quad \hat{\bs} - \bs^- - h(\bs^-,\bepsilon) &= \bs^+ - \hat{\bs} - h(\bs^+,\bepsilon) \\ 
    \therefore \quad \hat{\bs} &= \frac{1}{2}\left( \bs^- + \bs^+ +h(\bs^-,\bepsilon) - h(\bs^+,\bepsilon) \right).
\end{align}
Under this optimal choice of $\hat{\bs}$, 
\begin{equation*}
    2 \max g(\bs_\pm,\hat{\bs},\bepsilon) =  \bs^+  -  \bs^-  - h(\bs^-,\bepsilon) - h(\bs^+,\bepsilon).
\end{equation*}

If $\bs^- < 0 < \bs^+$, then \eqref{eq:h_abs_and_rel} cannot hold since $h(\bs,\bepsilon) \le \lvert \bs \rvert \bepsrel$, and so
\begin{equation*}
2 \max g(\bs^\pm,\hat{\bs},\bepsilon) \ge \bs^+ (1 - \bepsrel) - \bs^- (1  - \bepsrel) = (\bs^+ - \bs^-)(1 - \bepsrel) > 0.
\end{equation*}
If $\bs^- < 0 < \bs^+$ and $\bepsabs = 0$ for \eqref{eq:h_abs_or_rel}, then $h(\bs,\bepsilon) = \lvert \bs \rvert \bepsrel$, and by a similar argument, \eqref{eq:h_abs_and_rel} again cannot be satisfied.

\section{Additional QMCPy Features}

\AGSNote{
\begin{itemize}
    \item Multiple stopping criteria, both MC and QMC
    \item Variation reduction techniques such as
    \begin{itemize}
        \item importance sampling
        \item control variates
    \end{itemize}
    \item vectorized integrands $\boldsymbol{f}$
\end{itemize}
}

\section{Metalearning Studies}

\subsection{Gradient Boosted Trees}

\subsection{Neural Networks}


\section{Conclusions and Future Work}

\AGSNote{
\begin{itemize}
    \item Implement Tony's replicated designs when $U$ is compatible
\end{itemize}
}

\printbibliography

\end{document}
